import os
import sys
import logging
import time
import random
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization, LayerNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Ajouter le dossier racine du projet dans le path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# Import de la configuration centralis√©e des features
from src.utils.config_features import LSTM_FEATURES

# Configuration de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fixer la graine pour la reproductibilit√©
seed_value = 42
os.environ['PYTHONHASHSEED'] = str(seed_value)
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

def main():
    # Chemins absolus vers les fichiers de donn√©es pr√©trait√©es
    base_data_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../src/data"))
    X_train_path = os.path.join(base_data_path, "X_train.npy")
    y_train_path = os.path.join(base_data_path, "y_train.npy")
    X_val_path   = os.path.join(base_data_path, "X_val.npy")
    y_val_path   = os.path.join(base_data_path, "y_val.npy")
    
    # Chargement des donn√©es
    try:
        X_train = np.load(X_train_path)
        y_train = np.load(y_train_path)
        X_val   = np.load(X_val_path)
        y_val   = np.load(y_val_path)
    except Exception as e:
        logger.error(f"Erreur lors du chargement des donn√©es NumPy: {e}")
        return

    logger.info(f"‚úÖ Donn√©es charg√©es : X_train={X_train.shape}, X_val={X_val.shape}")

    # V√©rifier que le nombre de features correspond √† la configuration centralis√©e
    if X_train.shape[2] != len(LSTM_FEATURES):
        logger.error(f"Incoh√©rence des features : X_train a {X_train.shape[2]} features, mais LSTM_FEATURES contient {len(LSTM_FEATURES)}")
        raise ValueError("Incoh√©rence des features entre les donn√©es d'entra√Ænement et la configuration centrale")
    
    # D√©finition du mod√®le LSTM optimis√©
    model = Sequential([
        LSTM(256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dropout(0.4),
        BatchNormalization(),
        
        GRU(128, return_sequences=True),
        Dropout(0.3),
        BatchNormalization(),
        
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        LayerNormalization(),
        
        Dense(50, activation="relu"),
        Dense(25, activation="relu"),
        Dense(1, activation="linear")
    ])

    initial_lr = 0.001
    optimizer = Adam(learning_rate=initial_lr)
    model.compile(optimizer=optimizer, loss="mae")
    logger.info("Mod√®le compil√©.")

    # Chemin pour sauvegarder le checkpoint et le mod√®le final
    checkpoint_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../src/models/model_checkpoint.weights.h5"))
    final_model_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../src/models/LSTM_trading_model_final.keras"))

    # Callback pour early stopping et checkpointing
    early_stopping = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
    checkpoint_callback = ModelCheckpoint(
        filepath=checkpoint_path,
        monitor="val_loss",
        save_best_only=True,
        save_weights_only=True,
        verbose=1
    )

    # Charger les poids depuis le checkpoint s'il existe
    if os.path.exists(checkpoint_path):
        try:
            model.load_weights(checkpoint_path)
            logger.info("üîÑ Poids charg√©s depuis le checkpoint, reprise de l'entra√Ænement...")
        except Exception as e:
            logger.error(f"Erreur lors du chargement du checkpoint: {e}")

    # Entra√Ænement du mod√®le
    epochs = 100  # Vous pouvez ajuster ce nombre selon vos besoins
    batch_size = 128
    logger.info("üöÄ D√©but de l'entra√Ænement...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping, checkpoint_callback],
        verbose=2
    )
    
    # Sauvegarder le mod√®le final
    model.save(final_model_path)
    logger.info(f"‚úÖ Mod√®le final sauvegard√© sous '{final_model_path}'.")

if __name__ == "__main__":
    main()
